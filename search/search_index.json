{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Kubernetes Production Lab","text":"<p>Hello!, I'm Daniel Makhoba Emmanuel, a Kubernetes Engineer with years of experience in technology and a track record in Fullstack Engineering, DevOps and Cloud Native. </p> <p>I made this lab to not only upskill my kubernetes skills, but also document the process. In this Lab I experiment with different Opensource tools, AI, cloud-providers, essentially any technology I find and feel like \"Hey this would be pretty nice to test out or use in an organization\", and if it works I add it to my toolset.</p> <p>You can reach out through any of the following, even if it's to just make a new connection:</p> <ul> <li> LinkedIn</li> <li> Email</li> <li> Discord</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes and planned additions to this Lab will be documented in this file.</p>"},{"location":"CHANGELOG/#unreleased","title":"[Unreleased]","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Planning: <ul> <li>Implement HarshiCorp vault into Lab for secrets management.</li> <li>Conduct performs test on pods</li> <li>Migrate to GitOps friendly repo structure</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li> <p>Planning:</p> <ul> <li> <p>Migrate to <code>new-repo-branch</code> structure for Lab.</p> </li> <li> <p>Change to sealed secrets for secret management</p> </li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fix","title":"Fix","text":"<ul> <li>Planning: <ul> <li>Fix MongoDB in cluster deployment issue</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#10-07-2025-traefik-ingress","title":"[10-07-2025] - Traefik Ingress","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":""},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Migrated resources and markdown files to documentation site</li> <li>Updated README.md</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":""},{"location":"CHANGELOG/#10-07-2025-back-after-academic-break-now-a-graduate","title":"[10-07-2025] - Back after academic break (now a graduate!!)","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Migrated Documentation to github pages</li> </ul>"},{"location":"CHANGELOG/#changed_2","title":"Changed","text":""},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":""},{"location":"CHANGELOG/#26-06-2025-exposing-go-api-pods-with-ingress","title":"[26-06-2025] - Exposing Go API pods with ingress","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Added ingress resource for go pods(path based routing)</li> <li>Added seperate pipeline for hello backend</li> <li>Added hello backend manifests</li> <li>Added self signed tls certificate and key</li> </ul>"},{"location":"CHANGELOG/#changed_3","title":"Changed","text":"<ul> <li>Updated changelog</li> <li>Update hello backend and Go Api buid pipeline to filter changes in code related to the service</li> <li>Removed second environment until i'm ready to implement charts</li> <li>Added tls to path based ingress yaml</li> </ul>"},{"location":"CHANGELOG/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed pipeline running when code related to services where not affected</li> </ul>"},{"location":"CHANGELOG/#25-06-2025-exposing-go-api-pods-with-ingress","title":"[25-06-2025] - Exposing Go API pods with ingress","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li>Added ingress resource for go pods(host based routing)</li> <li>Added second service <code>hello-backend</code></li> </ul>"},{"location":"CHANGELOG/#changed_4","title":"Changed","text":"<ul> <li>Updated changelog</li> </ul>"},{"location":"CHANGELOG/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed failed pipeline due to expired credentials</li> </ul>"},{"location":"CHANGELOG/#20-06-2025-ingress","title":"[20-06-2025] - Ingress","text":""},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>Updated workloads in root <code>readme.md</code> </li> </ul>"},{"location":"CHANGELOG/#changed_5","title":"Changed","text":"<ul> <li>Updated changelog</li> <li>Updated k8s version</li> </ul>"},{"location":"CHANGELOG/#fixed_4","title":"Fixed","text":""},{"location":"CHANGELOG/#19-06-2025-fixed-crashing-pods","title":"[19-06-2025] - Fixed Crashing Pods","text":""},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Updated <code>problems-and-solutions-faced.md</code></li> <li>Added ingress-controller using helm</li> <li>Added ingress namespace</li> <li>Add Kubetail and expose it in the observability namespace</li> </ul>"},{"location":"CHANGELOG/#changed_6","title":"Changed","text":"<ul> <li>Updated changelog</li> <li>Split and update Architecture diagrams</li> </ul>"},{"location":"CHANGELOG/#fixed_5","title":"Fixed","text":"<ul> <li>Network Downtime somehow caused CoreDNS to be unable to reach the APIserver</li> </ul>"},{"location":"CHANGELOG/#18-06-2025-node-exporter","title":"[18-06-2025] - Node-Exporter","text":""},{"location":"CHANGELOG/#added_7","title":"Added","text":"<ul> <li>Added node-exporter daemonset to observability namespace</li> </ul>"},{"location":"CHANGELOG/#changed_7","title":"Changed","text":"<ul> <li>Updated changelog</li> <li>Updated architecture diagram</li> </ul>"},{"location":"CHANGELOG/#fixed_6","title":"Fixed","text":"<ul> <li>Narrowed down issue to permission and network(my ISP downtime), still fixing</li> </ul>"},{"location":"CHANGELOG/#17-06-2025-staging-pods-talking-to-dev-db","title":"[17-06-2025] - Staging Pods talking to Dev DB","text":""},{"location":"CHANGELOG/#added_8","title":"Added","text":"<ul> <li>Added dedicated credentials YAMl for staging environment</li> <li>Perform audit on manifests to consolidate standard</li> </ul>"},{"location":"CHANGELOG/#changed_8","title":"Changed","text":"<ul> <li>Updated changelog</li> </ul>"},{"location":"CHANGELOG/#fixed_7","title":"Fixed","text":"<ul> <li>Staging environment API pods we're calling the dev environment postgresql instance</li> </ul>"},{"location":"CHANGELOG/#16-06-2025-stateful-services-not-working-currently-fixing-configuration-issue","title":"[16-06-2025] - Stateful Services (Not working, currently fixing configuration issue)","text":""},{"location":"CHANGELOG/#added_9","title":"Added","text":"<ul> <li>MongoDB statefulset</li> <li>MongoDB headless-service</li> <li>MongoDB credentials</li> <li>MongoDB configMap</li> </ul>"},{"location":"CHANGELOG/#changed_9","title":"Changed","text":"<p>### Fixed - Fixed mismatch in credentials causing errors - Removed redundant init containercausing pod to fail</p>"},{"location":"CHANGELOG/#15-06-2025-updated-root-readme","title":"[15-06-2025] - Updated Root ReadME","text":""},{"location":"CHANGELOG/#added_10","title":"Added","text":""},{"location":"CHANGELOG/#changed_10","title":"Changed","text":"<ul> <li>Updated root readme to show current workloads</li> </ul>"},{"location":"CHANGELOG/#fixed_8","title":"Fixed","text":""},{"location":"CHANGELOG/#14-06-2025-stateful-services","title":"[14-06-2025] - Stateful Services","text":""},{"location":"CHANGELOG/#added_11","title":"Added","text":"<ul> <li>Added Redis stateful set</li> <li>Added Redis headless service</li> </ul>"},{"location":"CHANGELOG/#changed_11","title":"Changed","text":"<ul> <li>Updated architecture</li> </ul>"},{"location":"CHANGELOG/#fixed_9","title":"Fixed","text":""},{"location":"CHANGELOG/#13-06-2025-stateful-services","title":"[13-06-2025] - Stateful Services","text":""},{"location":"CHANGELOG/#added_12","title":"Added","text":"<ul> <li>Added mysql stateful set</li> <li>Added mysql headless service</li> <li>Added mysql secret</li> <li>Added go api credentials secrets to connect to cluster database</li> <li>Added postgres.go file to handle database connection</li> </ul>"},{"location":"CHANGELOG/#changed_12","title":"Changed","text":"<ul> <li>Updated architecture</li> <li>Removed db password secret and replaced</li> <li>Changed main.go to run database connection first</li> </ul>"},{"location":"CHANGELOG/#fixed_10","title":"Fixed","text":""},{"location":"CHANGELOG/#12-06-2025-stateful-services","title":"[12-06-2025] - Stateful Services","text":""},{"location":"CHANGELOG/#added_13","title":"Added","text":"<ul> <li>Added postgres stateful set</li> <li>Added postgres headless service</li> <li>Added postgres secret</li> </ul>"},{"location":"CHANGELOG/#changed_13","title":"Changed","text":"<ul> <li>Updated root ReadMe with instructions on navigating the repository</li> <li>Updated architecture</li> </ul>"},{"location":"CHANGELOG/#fixed_11","title":"Fixed","text":"<ul> <li>Fixed \"403 forbidden\" error on nginx stateful pods with an init container</li> </ul>"},{"location":"CHANGELOG/#10-06-2025-statefulsets","title":"[10-06-2025] - Statefulsets","text":""},{"location":"CHANGELOG/#added_14","title":"Added","text":"<ul> <li>Added Nginx statefulset</li> <li>Added Nginx headless service for statefulset</li> </ul>"},{"location":"CHANGELOG/#changed_14","title":"Changed","text":"<ul> <li>Removed nodeport yaml as its no longer needed</li> <li>Adjusted architcture image to include persistent volumes icons</li> </ul>"},{"location":"CHANGELOG/#fixed_12","title":"Fixed","text":""},{"location":"CHANGELOG/#09-06-2025-termination-grace-periods","title":"[09-06-2025] - Termination Grace Periods","text":""},{"location":"CHANGELOG/#added_15","title":"Added","text":""},{"location":"CHANGELOG/#changed_15","title":"Changed","text":""},{"location":"CHANGELOG/#fixed_13","title":"Fixed","text":"<ul> <li>Fixed pods taking too long to terminate by adding termination grace periods</li> </ul>"},{"location":"CHANGELOG/#07-06-2025-kube-daiagrams","title":"[07-06-2025] - Kube-daiagrams","text":""},{"location":"CHANGELOG/#added_16","title":"Added","text":"<ul> <li> <p>Added new tool kube-diagrams to generate architecture, saving more time </p> </li> <li> <p>Added Tools directory to show the tools being used in the lab</p> </li> <li> <p>Added Tools docs for kubetail and kube-diagram</p> </li> </ul>"},{"location":"CHANGELOG/#changed_16","title":"Changed","text":"<ul> <li>Removed old Architecture diagram in lab</li> </ul>"},{"location":"CHANGELOG/#fixed_14","title":"Fixed","text":"<ul> <li>Fixed deviatinf namespaces (dev and staging)</li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Hostpath manifests</li> </ul>"},{"location":"CHANGELOG/#02-06-2025-kubetail-and-nginx","title":"[02-06-2025] - Kubetail and nginx","text":""},{"location":"CHANGELOG/#added_17","title":"Added","text":"<ul> <li> <p>Added the reason for kubetail usage to the problem and solutions markdown</p> </li> <li> <p>Added sidecar container pod</p> </li> </ul>"},{"location":"CHANGELOG/#changed_17","title":"Changed","text":"<ul> <li> <p>Changed nginx service to communicate with existing deployment and new sidecar container pod</p> </li> <li> <p>Removed basic pod.yaml</p> </li> </ul>"},{"location":"CHANGELOG/#fixed_15","title":"Fixed","text":""},{"location":"CHANGELOG/#01-06-2025-observability-and-probes","title":"[01-06-2025] - Observability and Probes","text":""},{"location":"CHANGELOG/#added_18","title":"Added","text":"<ul> <li>Added Observability namespace.</li> <li>Created <code>/health</code> endpoint for liveness probe</li> <li>Added Liveness probe for Go API pod</li> <li>Created Readiness Probe for GO API pods probes.</li> <li>Created <code>/ready</code> endpoint for readiness probe</li> </ul>"},{"location":"CHANGELOG/#changed_18","title":"Changed","text":"<ul> <li>Modified go api deployment to include liveness probe</li> </ul>"},{"location":"CHANGELOG/#fixed_16","title":"Fixed","text":""},{"location":"CHANGELOG/#30-05-2025-changelog-initialization","title":"[30-05-2025] - ChangeLog Initialization","text":""},{"location":"CHANGELOG/#added_19","title":"Added","text":"<ul> <li>Initial <code>CHANGELOG.md</code> created.</li> </ul>"},{"location":"CHANGELOG/#changed_19","title":"Changed","text":"<ul> <li>Updated Deployment files to inject all environment variables from a ConfigMap into pods, instead of individual key-value pairs.</li> </ul>"},{"location":"CHANGELOG/#fixed_17","title":"Fixed","text":""},{"location":"Problems-and-solutions-faced/","title":"Problems and Solutions Faced","text":""},{"location":"Problems-and-solutions-faced/#1-workload-in-control-plane","title":"1. Workload in Control Plane","text":"<p>The lab uses KIND(Kubernetes in Docker) to deploy k8s, and by default in a standard enviroment (contol plane and worker nodes), a taint is applied on the control plane to prevent workloads without the required tolerance from being deployed there (in cloud setups it's the same). However, since i only had one node and that was the control-plane this taint wasn't applied and workloads were being deployed into it.</p>"},{"location":"Problems-and-solutions-faced/#solution","title":"Solution","text":"<p>To solve this I made a config.yaml with a control plane and worker nodes. The old plane was deleted and this config.yaml deployed. The taint for the control-plane was then able to take effect and all workloads were deployed to the worker nodes.</p>"},{"location":"Problems-and-solutions-faced/#2-too-many-logs-to-monitor","title":"2. Too many Logs to monitor","text":"<p>As the pod count kept on increasing, so had the difficulty when monitoring the logs in real time.</p>"},{"location":"Problems-and-solutions-faced/#solution_1","title":"Solution","text":"<p>I found a tool called kubetail that allows uses a web dashboard to to display all the logs in my cluster in real-time. I can filter logs by time, pod, namespace and node. And i can also switch between local and remote clusters making it ideal for porduction. I gave a breakdown on this here https://x.com/DanielMakhoba_E/status/1929112097272557761</p>"},{"location":"Problems-and-solutions-faced/#3-nginx-path-hit-and-miss","title":"3. Nginx path hit and miss","text":"<p>When experimenting with sidecar containers (web server(nginx) + logger) and a seperate nginx-deployment. The the sidecar contianer port had a different path \"/app.log\" and the deployments path had a \"/\" and \"/custom\" path. When performing curls to the service, it would sometime show the result and, sometimes a forbidden or not found error.</p> <p></p>"},{"location":"Problems-and-solutions-faced/#solution_2","title":"Solution","text":"<p>This highlighted how the service object works. It is not aware of which pods have a particular path rather it merely checks if they'ready and sends traffic , which can result in a miss or a hit. To solve i would use an ingress controller to enable path absed routing</p>"},{"location":"Problems-and-solutions-faced/#4-maintaining-architecture","title":"4. Maintaining architecture","text":"<p>As the Lab grows, the number of elements (Clusters, namespaces, replicasets, pods etc.) keep increasing, making it harder to keep track of the architecture, and wastes time trying to develop it.</p>"},{"location":"Problems-and-solutions-faced/#solution_3","title":"Solution","text":"<p>I found a tool called KubeDiagrams which solves this by using the manifests,helm, and kubectl to determine the state of the environment. KubeDiagrams can also be integrated into CI/CD pipelines and is suitable</p> <p></p>"},{"location":"Problems-and-solutions-faced/#5-termination-period-taking-too-long","title":"5. Termination Period taking too long","text":"<p>One of my pods was taking too long to terminate, although it eventually did, the possibility that it wouldn't always terminate on-time still existed</p>"},{"location":"Problems-and-solutions-faced/#solution_4","title":"solution","text":"<p>To prevent this i added a termination grace period cluster-wide on all deployments</p>"},{"location":"Problems-and-solutions-faced/#6-stateful-postgres-error-residual-data-from-pvc","title":"6. Stateful Postgres Error - Residual data from PVC","text":"<p>I made an error with an init container in the pod. It was writing a txt file to the path of the volumeMount. The pod went into a crashBackLoop. </p>"},{"location":"Problems-and-solutions-faced/#solution_5","title":"Solution","text":"<p>I didn't realize that postgres runs an initDB on that path to check if it's empty. I fixed that by deleting the init container, but I was still getting the same error. I realized that the pvc (with the edited path) used was still available and the pod was connecting to it. I deleted the old pvc and re-ran the manifest, which successfully provisioned the pod.</p>"},{"location":"Problems-and-solutions-faced/#7-staging-environment-pods-connecting-to-wrong-database","title":"7. Staging Environment Pods connecting to wrong Database","text":"<p>The pods in the staging environment were using the dev environments postgresql instance instead of their envionment's. This misconfiguration didn't lead to any problems, but was found when i was auiting the yaml files and noticed it.</p>"},{"location":"Problems-and-solutions-faced/#solution_6","title":"Solution","text":"<p>For the Go API pods to connect to the postgresql databases , the FQDN for the service has to be used. This FQDN <code>postgres-headless-service.dev.svc.cluster.local</code> was specified in the <code>go-api-credentials</code> yaml initially. The issue however was there are two environments. Meaning if this yaml was applied as well in the staging environment it would point to the dev environments POstgres instance, ignoring its own. </p> <p>To fix this I seperated the go-api-credentials yaml into two <code>go-api-credentials-dev</code> and <code>go-api-credentials-staging</code></p>"},{"location":"Problems-and-solutions-faced/#8-coredns-caused-a-cluster-wide-failure","title":"8. CoreDNS Caused a Cluster-wide Failure","text":"<p>The Go API Pods across the cluster were stuck in a CrashBackLoop cycle. While some might occasionally connect briefly, the majority consistently failed to establish a database connection. At the same time my ISP eas experiencing a major downtime in my region.</p>"},{"location":"Problems-and-solutions-faced/#solution_7","title":"Solution","text":"<p>My initial troubleshooting involved deleting the Go API deployment and re-deploying it using a different ISP. This, however, led to the same issue, ruling out the external network as the sole cause. The Go API pod logs, while initially confusing, consistently showed a <code>failed to connect to postgres-headless-service.dev.svc.cluster.local</code> error, specifically indicating a problem resolving the database service's IP address. This was strange, since the PostgreSQL database pod and its headless service were working fine.</p> <p>I decided to test the connectivity from within the cluster using a busybox pod. Running <code>nc -vz postgres-headless-service.dev.svc.cluster.local 5432</code> from the busybox pod. This returned a \"bad address\" error. This confirmed that DNS resolution for the database service was failing from inside the cluster, indicating CoreDNS as the potential issue.</p> <p>After inspecting the CoreDNS pods in the kube-system namespace, they initially appeared Running. However, diving deeper into their logs revealed critical errors: CoreDNS was repeatedly failing to connect to the Kubernetes API server (the kubernetes service at 10.96.0.1:443) with \"dial tcp i/o timeout\" messages. This meant CoreDNS was \"blind\" to the cluster's services and endpoints, hence unable to resolve service names.</p> <p>This pointed to a fundamental, cluster-wide network breakdown, likely stemming from a corrupted state in Kind's underlying Docker network bridge hightened by the recent external network issues. Given the severity of the problem, a full cluster reset was done. After a clean recreation of the Kind cluster and redeploying my applications, all core components became stable, and the Go API pods successfully connected to the PostgreSQL database</p>"},{"location":"Problems-and-solutions-faced/#9-pipeline-keeps-building-on-every-commit-the-irony","title":"9. Pipeline keeps building on every commit (the irony!)","text":"<p>Everytime I made a commit regardless of if it affected the the dockerfiles or backend code, or not the pipeline would run.</p>"},{"location":"Problems-and-solutions-faced/#solution_8","title":"Solution","text":"<p>This was due to my faulty design of the pipeline. I fixed the pipeline to only run when the code involving the deployed services were changed</p>"},{"location":"openSource_Tools_used/","title":"Tools","text":""},{"location":"openSource_Tools_used/#observability","title":"OBSERVABILITY","text":"<ul> <li> <p>KubeTail: Real-time Log-streaming</p> </li> <li> <p>KubeDiagrams: Automated Architecture Diagrams</p> </li> </ul>"},{"location":"lab-setup/architecture/","title":"Architectures","text":"<p>My architecture diagrams are automated using an opensource tool called kube-Diagrams. Kube-Diagrams allows to really specify what i want in the diagram and generates it. You can checkout their official repo on GitHub.</p>"},{"location":"lab-setup/architecture/#developmentdev-namespace","title":"Development(Dev) Namespace","text":""},{"location":"lab-setup/architecture/#ingressnginx-namespace","title":"Ingress(Nginx) Namespace","text":""},{"location":"lab-setup/architecture/#observability-namespace","title":"Observability Namespace","text":""}]}